{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7055e9a7-dd6d-419d-b2fa-cdd086f03e5c",
   "metadata": {},
   "source": [
    "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
    "# <font color='blue'>Data Science Academy</font>\n",
    "## <font color='blue'>PySpark e Apache Kafka Para Processamento de Dados em Batch e Streaming</font>\n",
    "## <font color='blue'>Projeto 8</font>\n",
    "### <font color='blue'>Simulação de Erros e Recuperação de Falhas em Multi-Node Kafka Cluster</font>\n",
    "### <font color='blue'>Kafka Consumer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327097b-87cf-41a0-a082-d86d18bc233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, sum, to_timestamp, window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afec34c-d58b-4d13-b678-473f5e2c9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a sessão Spark com as classes para o Kafka\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DSAProjeto8\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.apache.kafka:kafka-clients:2.8.0\") \\\n",
    "    .config(\"spark.streaming.kafka.consumer.cache.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0c5c4-e3af-4501-a441-f19ad86ffdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altera o nível de log para ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b2c5c-e946-4c08-ae74-1795c7a9cb70",
   "metadata": {},
   "source": [
    "## Leitura do Stream do Kafka em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc5d5c-6b8b-4e00-b414-6675d0695236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o schema para os dados\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"id_produto\", IntegerType()),\n",
    "    StructField(\"quantidade\", IntegerType()),\n",
    "    StructField(\"preco\", DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c9387-ad4f-4030-8c65-3cd5c991d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai os nomes dos brokers\n",
    "kafka_broker = os.environ.get('KAFKA_BROKER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022f1da-2a68-42af-a004-da2cce5f962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome do tópico\n",
    "kafka_topic = \"dsa_p8_topico\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63119a-f580-4a13-8fe5-db25b744c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o readStream para ler os dados do Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.security.protocol\", \"PLAINTEXT\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3750c94-bea7-4480-92cc-66b35182ea2b",
   "metadata": {},
   "source": [
    "## Processamento do Stream com PySpark em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20431623-8788-4477-a148-a344cd4707ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse do formato JSON para preparar o dataframe\n",
    "parsed_df = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacfa8cb-ef81-46a2-be18-13cdebb02605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte o registro de data e hora para o formato adequado e calcula o total acumulado para 1 dia \n",
    "# (você pode alterar a janela para o período de sua preferência)\n",
    "df_vendas = parsed_df \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(\"timestamp\")) \\\n",
    "    .withColumn(\"vendas\", col(\"quantidade\") * col(\"preco\")) \\\n",
    "    .groupBy(window(\"timestamp\", \"1 day\")) \\\n",
    "    .agg(sum(\"vendas\").alias(\"total_acumulado\")) \\\n",
    "    .select(col(\"window.end\").alias(\"window_end\"), col(\"total_acumulado\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000e565-aafb-40fe-b371-c196747fc82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis de conexão do PostgreSQL\n",
    "pg_host = os.environ.get('POSTGRES_HOST')\n",
    "pg_db = os.environ.get('POSTGRES_DB')\n",
    "pg_user = os.environ.get('POSTGRES_USER')\n",
    "pg_password = os.environ.get('POSTGRES_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f90563-721e-444a-a905-0454640d971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para salvar o stream de dados no banco de dados (após o processamento)\n",
    "def dsa_salva_stream_database(df, epoch_id):\n",
    "\n",
    "    # Cria conexão ao banco de dados\n",
    "    conn = psycopg2.connect(host = pg_host, \n",
    "                            database = pg_db, \n",
    "                            user = pg_user, \n",
    "                            password = pg_password)\n",
    "\n",
    "    # Cria o cursor\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Cria a tabela no banco de dados (se não existir)\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dsa_total_acumulado (\n",
    "        timestamp TIMESTAMP PRIMARY KEY,\n",
    "        total_acumulado_real_time DOUBLE PRECISION\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"\\n--- Total Acumulado Atualizado em {datetime.now()} ---\\n\")\n",
    "    print(\"Timestamp | Total Acumulado Real Time\")\n",
    "    print(\"-----------+---------------\")\n",
    "\n",
    "    # Loop pelas linhas extraídas do stream de dados\n",
    "    for row in df.collect():\n",
    "\n",
    "        # window_end se refere ao timestamp que marca o final de uma janela de tempo usada no processamento de streaming.\n",
    "        # Ele é gerado automaticamente pelo Spark ao realizar agregações em janelas de tempo.\n",
    "        window_end = row.window_end\n",
    "\n",
    "        # Como não especificamos o tamanho da janela, cada evento gerado será acumulado com o próximo gerando o valor agregado.\n",
    "        total_acumulado = row.total_acumulado\n",
    "\n",
    "        # Se diferente de None\n",
    "        if window_end is not None and total_acumulado is not None:\n",
    "\n",
    "            # Insere os dados na tabela\n",
    "            cur.execute(\"\"\"\n",
    "            INSERT INTO dsa_total_acumulado (timestamp, total_acumulado_real_time)\n",
    "            VALUES (%s, %s)\n",
    "            ON CONFLICT (timestamp) DO UPDATE\n",
    "            SET total_acumulado_real_time = EXCLUDED.total_acumulado_real_time\n",
    "            \"\"\", (window_end, total_acumulado))\n",
    "            \n",
    "            print(f\"{window_end} | {total_acumulado:.2f}\")\n",
    "        else:\n",
    "            print(f\"Ignorando linha devido a valores None: window_end = {window_end}, total_acumulado = {total_acumulado}\")\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0ee4d-064a-45d4-a5d0-b3fe2ed92ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia a extração do writeStream e gravação no banco de dados\n",
    "query = df_vendas.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(dsa_salva_stream_database) \\\n",
    "    .trigger(processingTime = '10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483511d9-b9a2-41f2-94ba-4b0810df6d5f",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
